{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca5d24e",
   "metadata": {},
   "source": [
    "<p1>To use MLP, CNN, RNN, RNN with LSTM</p1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ac60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f4c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess the data\n",
    "def PreProc(X, Y):\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Columns_Scaled = scaler.transform(X)\n",
    "    pca = PCA(n_components = 1000).fit(Columns_Scaled).transform(Columns_Scaled)\n",
    "    Columns_Scaled = pca\n",
    "    return Columns_Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "030df340-3d97-4e32-92f4-d0ff63346c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.620</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.60</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.700</td>\n",
       "      <td>2.060</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>23.50</td>\n",
       "      <td>20.300</td>\n",
       "      <td>20.300</td>\n",
       "      <td>23.50</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.800</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.80</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.880</td>\n",
       "      <td>3.830</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-23.30</td>\n",
       "      <td>-21.800</td>\n",
       "      <td>-21.800</td>\n",
       "      <td>-23.30</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.900</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.70</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.200</td>\n",
       "      <td>89.900</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>462.00</td>\n",
       "      <td>-233.000</td>\n",
       "      <td>-233.000</td>\n",
       "      <td>462.00</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.900</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.80</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.820</td>\n",
       "      <td>2.300</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>299.00</td>\n",
       "      <td>-243.000</td>\n",
       "      <td>-243.000</td>\n",
       "      <td>299.00</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.300</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.30</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.060</td>\n",
       "      <td>41.400</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>38.100</td>\n",
       "      <td>38.100</td>\n",
       "      <td>12.00</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>32.400</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>30.80</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1.640</td>\n",
       "      <td>-2.030</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.70</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-21.70</td>\n",
       "      <td>95.2</td>\n",
       "      <td>-19.90</td>\n",
       "      <td>47.20</td>\n",
       "      <td>47.20</td>\n",
       "      <td>-19.90</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>16.300</td>\n",
       "      <td>31.3</td>\n",
       "      <td>-284.0</td>\n",
       "      <td>14.30</td>\n",
       "      <td>23.9</td>\n",
       "      <td>4.200</td>\n",
       "      <td>1.090</td>\n",
       "      <td>4.460</td>\n",
       "      <td>4.720</td>\n",
       "      <td>6.63</td>\n",
       "      <td>...</td>\n",
       "      <td>594.00</td>\n",
       "      <td>-324.000</td>\n",
       "      <td>-324.000</td>\n",
       "      <td>594.00</td>\n",
       "      <td>-35.5</td>\n",
       "      <td>142.00</td>\n",
       "      <td>-59.80</td>\n",
       "      <td>-59.80</td>\n",
       "      <td>142.00</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>-0.547</td>\n",
       "      <td>28.3</td>\n",
       "      <td>-259.0</td>\n",
       "      <td>15.80</td>\n",
       "      <td>26.7</td>\n",
       "      <td>9.080</td>\n",
       "      <td>6.900</td>\n",
       "      <td>12.700</td>\n",
       "      <td>2.030</td>\n",
       "      <td>4.64</td>\n",
       "      <td>...</td>\n",
       "      <td>370.00</td>\n",
       "      <td>-160.000</td>\n",
       "      <td>-160.000</td>\n",
       "      <td>370.00</td>\n",
       "      <td>408.0</td>\n",
       "      <td>-169.00</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>-169.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>16.800</td>\n",
       "      <td>19.9</td>\n",
       "      <td>-288.0</td>\n",
       "      <td>8.34</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.460</td>\n",
       "      <td>1.580</td>\n",
       "      <td>-16.000</td>\n",
       "      <td>1.690</td>\n",
       "      <td>4.74</td>\n",
       "      <td>...</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-27.600</td>\n",
       "      <td>-27.600</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-656.0</td>\n",
       "      <td>552.00</td>\n",
       "      <td>-271.00</td>\n",
       "      <td>-271.00</td>\n",
       "      <td>552.00</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>27.000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>25.00</td>\n",
       "      <td>28.9</td>\n",
       "      <td>4.990</td>\n",
       "      <td>1.950</td>\n",
       "      <td>6.210</td>\n",
       "      <td>3.490</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>...</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.95</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>22.80</td>\n",
       "      <td>22.80</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2132 rows × 2549 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  \\\n",
       "0          4.620      30.3    -356.0     15.60      26.3       1.070   \n",
       "1         28.800      33.1      32.0     25.80      22.8       6.550   \n",
       "2          8.900      29.4    -416.0     16.70      23.7      79.900   \n",
       "3         14.900      31.6    -143.0     19.80      24.3      -0.584   \n",
       "4         28.300      31.3      45.2     27.30      24.5      34.800   \n",
       "...          ...       ...       ...       ...       ...         ...   \n",
       "2127      32.400      32.2      32.2     30.80      23.4       1.640   \n",
       "2128      16.300      31.3    -284.0     14.30      23.9       4.200   \n",
       "2129      -0.547      28.3    -259.0     15.80      26.7       9.080   \n",
       "2130      16.800      19.9    -288.0      8.34      26.0       2.460   \n",
       "2131      27.000      32.0      31.8     25.00      28.9       4.990   \n",
       "\n",
       "      mean_d_1_a  mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_741_b  \\\n",
       "0          0.411     -15.700       2.060        3.15  ...      23.50   \n",
       "1          1.680       2.880       3.830       -4.82  ...     -23.30   \n",
       "2          3.360      90.200      89.900        2.03  ...     462.00   \n",
       "3         -0.284       8.820       2.300       -1.97  ...     299.00   \n",
       "4         -5.790       3.060      41.400        5.52  ...      12.00   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2127      -2.030       0.647      -0.121       -1.10  ...     -21.70   \n",
       "2128       1.090       4.460       4.720        6.63  ...     594.00   \n",
       "2129       6.900      12.700       2.030        4.64  ...     370.00   \n",
       "2130       1.580     -16.000       1.690        4.74  ...     124.00   \n",
       "2131       1.950       6.210       3.490       -3.51  ...       1.95   \n",
       "\n",
       "      fft_742_b  fft_743_b  fft_744_b  fft_745_b  fft_746_b  fft_747_b  \\\n",
       "0        20.300     20.300      23.50     -215.0     280.00    -162.00   \n",
       "1       -21.800    -21.800     -23.30      182.0       2.57     -31.60   \n",
       "2      -233.000   -233.000     462.00     -267.0     281.00    -148.00   \n",
       "3      -243.000   -243.000     299.00      132.0     -12.40       9.53   \n",
       "4        38.100     38.100      12.00      119.0     -17.60      23.90   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2127      0.218      0.218     -21.70       95.2     -19.90      47.20   \n",
       "2128   -324.000   -324.000     594.00      -35.5     142.00     -59.80   \n",
       "2129   -160.000   -160.000     370.00      408.0    -169.00     -10.50   \n",
       "2130    -27.600    -27.600     124.00     -656.0     552.00    -271.00   \n",
       "2131      1.810      1.810       1.95      110.0      -6.71      22.80   \n",
       "\n",
       "      fft_748_b  fft_749_b     label  \n",
       "0       -162.00     280.00  NEGATIVE  \n",
       "1        -31.60       2.57   NEUTRAL  \n",
       "2       -148.00     281.00  POSITIVE  \n",
       "3          9.53     -12.40  POSITIVE  \n",
       "4         23.90     -17.60   NEUTRAL  \n",
       "...         ...        ...       ...  \n",
       "2127      47.20     -19.90   NEUTRAL  \n",
       "2128     -59.80     142.00  POSITIVE  \n",
       "2129     -10.50    -169.00  NEGATIVE  \n",
       "2130    -271.00     552.00  NEGATIVE  \n",
       "2131      22.80      -6.71   NEUTRAL  \n",
       "\n",
       "[2132 rows x 2549 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"emotions.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4f19ad-b088-4e37-96ef-67cad291476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.drop(\"label\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34027a64-02c4-4cbc-b447-8733bb367190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># mean_0_a</th>\n",
       "      <th>mean_1_a</th>\n",
       "      <th>mean_2_a</th>\n",
       "      <th>mean_3_a</th>\n",
       "      <th>mean_4_a</th>\n",
       "      <th>mean_d_0_a</th>\n",
       "      <th>mean_d_1_a</th>\n",
       "      <th>mean_d_2_a</th>\n",
       "      <th>mean_d_3_a</th>\n",
       "      <th>mean_d_4_a</th>\n",
       "      <th>...</th>\n",
       "      <th>fft_740_b</th>\n",
       "      <th>fft_741_b</th>\n",
       "      <th>fft_742_b</th>\n",
       "      <th>fft_743_b</th>\n",
       "      <th>fft_744_b</th>\n",
       "      <th>fft_745_b</th>\n",
       "      <th>fft_746_b</th>\n",
       "      <th>fft_747_b</th>\n",
       "      <th>fft_748_b</th>\n",
       "      <th>fft_749_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.620</td>\n",
       "      <td>30.3</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>15.60</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1.070</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-15.700</td>\n",
       "      <td>2.060</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>74.3</td>\n",
       "      <td>23.50</td>\n",
       "      <td>20.300</td>\n",
       "      <td>20.300</td>\n",
       "      <td>23.50</td>\n",
       "      <td>-215.0</td>\n",
       "      <td>280.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>-162.00</td>\n",
       "      <td>280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.800</td>\n",
       "      <td>33.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.80</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.550</td>\n",
       "      <td>1.680</td>\n",
       "      <td>2.880</td>\n",
       "      <td>3.830</td>\n",
       "      <td>-4.82</td>\n",
       "      <td>...</td>\n",
       "      <td>130.0</td>\n",
       "      <td>-23.30</td>\n",
       "      <td>-21.800</td>\n",
       "      <td>-21.800</td>\n",
       "      <td>-23.30</td>\n",
       "      <td>182.0</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>-31.60</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.900</td>\n",
       "      <td>29.4</td>\n",
       "      <td>-416.0</td>\n",
       "      <td>16.70</td>\n",
       "      <td>23.7</td>\n",
       "      <td>79.900</td>\n",
       "      <td>3.360</td>\n",
       "      <td>90.200</td>\n",
       "      <td>89.900</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>-534.0</td>\n",
       "      <td>462.00</td>\n",
       "      <td>-233.000</td>\n",
       "      <td>-233.000</td>\n",
       "      <td>462.00</td>\n",
       "      <td>-267.0</td>\n",
       "      <td>281.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>-148.00</td>\n",
       "      <td>281.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.900</td>\n",
       "      <td>31.6</td>\n",
       "      <td>-143.0</td>\n",
       "      <td>19.80</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>8.820</td>\n",
       "      <td>2.300</td>\n",
       "      <td>-1.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-183.0</td>\n",
       "      <td>299.00</td>\n",
       "      <td>-243.000</td>\n",
       "      <td>-243.000</td>\n",
       "      <td>299.00</td>\n",
       "      <td>132.0</td>\n",
       "      <td>-12.40</td>\n",
       "      <td>9.53</td>\n",
       "      <td>9.53</td>\n",
       "      <td>-12.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.300</td>\n",
       "      <td>31.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>27.30</td>\n",
       "      <td>24.5</td>\n",
       "      <td>34.800</td>\n",
       "      <td>-5.790</td>\n",
       "      <td>3.060</td>\n",
       "      <td>41.400</td>\n",
       "      <td>5.52</td>\n",
       "      <td>...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>38.100</td>\n",
       "      <td>38.100</td>\n",
       "      <td>12.00</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-17.60</td>\n",
       "      <td>23.90</td>\n",
       "      <td>23.90</td>\n",
       "      <td>-17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>32.400</td>\n",
       "      <td>32.2</td>\n",
       "      <td>32.2</td>\n",
       "      <td>30.80</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1.640</td>\n",
       "      <td>-2.030</td>\n",
       "      <td>0.647</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>...</td>\n",
       "      <td>155.0</td>\n",
       "      <td>-21.70</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.218</td>\n",
       "      <td>-21.70</td>\n",
       "      <td>95.2</td>\n",
       "      <td>-19.90</td>\n",
       "      <td>47.20</td>\n",
       "      <td>47.20</td>\n",
       "      <td>-19.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2128</th>\n",
       "      <td>16.300</td>\n",
       "      <td>31.3</td>\n",
       "      <td>-284.0</td>\n",
       "      <td>14.30</td>\n",
       "      <td>23.9</td>\n",
       "      <td>4.200</td>\n",
       "      <td>1.090</td>\n",
       "      <td>4.460</td>\n",
       "      <td>4.720</td>\n",
       "      <td>6.63</td>\n",
       "      <td>...</td>\n",
       "      <td>-661.0</td>\n",
       "      <td>594.00</td>\n",
       "      <td>-324.000</td>\n",
       "      <td>-324.000</td>\n",
       "      <td>594.00</td>\n",
       "      <td>-35.5</td>\n",
       "      <td>142.00</td>\n",
       "      <td>-59.80</td>\n",
       "      <td>-59.80</td>\n",
       "      <td>142.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2129</th>\n",
       "      <td>-0.547</td>\n",
       "      <td>28.3</td>\n",
       "      <td>-259.0</td>\n",
       "      <td>15.80</td>\n",
       "      <td>26.7</td>\n",
       "      <td>9.080</td>\n",
       "      <td>6.900</td>\n",
       "      <td>12.700</td>\n",
       "      <td>2.030</td>\n",
       "      <td>4.64</td>\n",
       "      <td>...</td>\n",
       "      <td>-232.0</td>\n",
       "      <td>370.00</td>\n",
       "      <td>-160.000</td>\n",
       "      <td>-160.000</td>\n",
       "      <td>370.00</td>\n",
       "      <td>408.0</td>\n",
       "      <td>-169.00</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>-10.50</td>\n",
       "      <td>-169.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>16.800</td>\n",
       "      <td>19.9</td>\n",
       "      <td>-288.0</td>\n",
       "      <td>8.34</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.460</td>\n",
       "      <td>1.580</td>\n",
       "      <td>-16.000</td>\n",
       "      <td>1.690</td>\n",
       "      <td>4.74</td>\n",
       "      <td>...</td>\n",
       "      <td>-99.7</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-27.600</td>\n",
       "      <td>-27.600</td>\n",
       "      <td>124.00</td>\n",
       "      <td>-656.0</td>\n",
       "      <td>552.00</td>\n",
       "      <td>-271.00</td>\n",
       "      <td>-271.00</td>\n",
       "      <td>552.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>27.000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>25.00</td>\n",
       "      <td>28.9</td>\n",
       "      <td>4.990</td>\n",
       "      <td>1.950</td>\n",
       "      <td>6.210</td>\n",
       "      <td>3.490</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>...</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.810</td>\n",
       "      <td>1.95</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>22.80</td>\n",
       "      <td>22.80</td>\n",
       "      <td>-6.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2132 rows × 2548 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      # mean_0_a  mean_1_a  mean_2_a  mean_3_a  mean_4_a  mean_d_0_a  \\\n",
       "0          4.620      30.3    -356.0     15.60      26.3       1.070   \n",
       "1         28.800      33.1      32.0     25.80      22.8       6.550   \n",
       "2          8.900      29.4    -416.0     16.70      23.7      79.900   \n",
       "3         14.900      31.6    -143.0     19.80      24.3      -0.584   \n",
       "4         28.300      31.3      45.2     27.30      24.5      34.800   \n",
       "...          ...       ...       ...       ...       ...         ...   \n",
       "2127      32.400      32.2      32.2     30.80      23.4       1.640   \n",
       "2128      16.300      31.3    -284.0     14.30      23.9       4.200   \n",
       "2129      -0.547      28.3    -259.0     15.80      26.7       9.080   \n",
       "2130      16.800      19.9    -288.0      8.34      26.0       2.460   \n",
       "2131      27.000      32.0      31.8     25.00      28.9       4.990   \n",
       "\n",
       "      mean_d_1_a  mean_d_2_a  mean_d_3_a  mean_d_4_a  ...  fft_740_b  \\\n",
       "0          0.411     -15.700       2.060        3.15  ...       74.3   \n",
       "1          1.680       2.880       3.830       -4.82  ...      130.0   \n",
       "2          3.360      90.200      89.900        2.03  ...     -534.0   \n",
       "3         -0.284       8.820       2.300       -1.97  ...     -183.0   \n",
       "4         -5.790       3.060      41.400        5.52  ...      114.0   \n",
       "...          ...         ...         ...         ...  ...        ...   \n",
       "2127      -2.030       0.647      -0.121       -1.10  ...      155.0   \n",
       "2128       1.090       4.460       4.720        6.63  ...     -661.0   \n",
       "2129       6.900      12.700       2.030        4.64  ...     -232.0   \n",
       "2130       1.580     -16.000       1.690        4.74  ...      -99.7   \n",
       "2131       1.950       6.210       3.490       -3.51  ...      181.0   \n",
       "\n",
       "      fft_741_b  fft_742_b  fft_743_b  fft_744_b  fft_745_b  fft_746_b  \\\n",
       "0         23.50     20.300     20.300      23.50     -215.0     280.00   \n",
       "1        -23.30    -21.800    -21.800     -23.30      182.0       2.57   \n",
       "2        462.00   -233.000   -233.000     462.00     -267.0     281.00   \n",
       "3        299.00   -243.000   -243.000     299.00      132.0     -12.40   \n",
       "4         12.00     38.100     38.100      12.00      119.0     -17.60   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2127     -21.70      0.218      0.218     -21.70       95.2     -19.90   \n",
       "2128     594.00   -324.000   -324.000     594.00      -35.5     142.00   \n",
       "2129     370.00   -160.000   -160.000     370.00      408.0    -169.00   \n",
       "2130     124.00    -27.600    -27.600     124.00     -656.0     552.00   \n",
       "2131       1.95      1.810      1.810       1.95      110.0      -6.71   \n",
       "\n",
       "      fft_747_b  fft_748_b  fft_749_b  \n",
       "0       -162.00    -162.00     280.00  \n",
       "1        -31.60     -31.60       2.57  \n",
       "2       -148.00    -148.00     281.00  \n",
       "3          9.53       9.53     -12.40  \n",
       "4         23.90      23.90     -17.60  \n",
       "...         ...        ...        ...  \n",
       "2127      47.20      47.20     -19.90  \n",
       "2128     -59.80     -59.80     142.00  \n",
       "2129     -10.50     -10.50    -169.00  \n",
       "2130    -271.00    -271.00     552.00  \n",
       "2131      22.80      22.80      -6.71  \n",
       "\n",
       "[2132 rows x 2548 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfb0254-b3c9-46a6-8bde-38666ae71b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2132, 2549)\n",
      "(2132, 2548)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(columns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f130ce02-fe8d-4f7a-a7bd-e3cbb39d44b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NEGATIVE\n",
       "1        NEUTRAL\n",
       "2       POSITIVE\n",
       "3       POSITIVE\n",
       "4        NEUTRAL\n",
       "          ...   \n",
       "2127     NEUTRAL\n",
       "2128    POSITIVE\n",
       "2129    NEGATIVE\n",
       "2130    NEGATIVE\n",
       "2131     NEUTRAL\n",
       "Name: label, Length: 2132, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df.label\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e693ea3-ae20-4428-a3da-077913d3d2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaust\\AppData\\Local\\Temp\\ipykernel_25468\\2636447299.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y = y.replace([\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"], [0,1,2])\n"
     ]
    }
   ],
   "source": [
    "y = y.replace([\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"], [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6dd6ab3-0fcf-4d8e-8265-00c1c0d5b585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       2\n",
       "3       2\n",
       "4       1\n",
       "       ..\n",
       "2127    1\n",
       "2128    2\n",
       "2129    0\n",
       "2130    0\n",
       "2131    1\n",
       "Name: label, Length: 2132, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e63593-b276-4bea-89e7-8f5f1b38b220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.56578006e+01, -2.25872122e+01, -2.28763294e+01, ...,\n",
       "         3.18346979e-02, -4.50240627e-02,  7.18766788e-02],\n",
       "       [ 2.18855795e+01,  2.02972909e+00,  3.42903498e-02, ...,\n",
       "         7.62245334e-02, -1.70614440e-02, -1.60002617e-01],\n",
       "       [-3.52740527e+01,  1.48733177e+01,  5.62362963e+00, ...,\n",
       "        -2.83554332e-02, -7.95416110e-02,  1.35181112e-02],\n",
       "       ...,\n",
       "       [-1.97379003e+01, -1.53993196e+01,  3.52973555e+01, ...,\n",
       "         6.57679405e-02,  9.59383423e-02,  1.74930186e-01],\n",
       "       [-2.70723509e+01,  3.38604572e+01, -3.53183471e+01, ...,\n",
       "        -1.81091487e-01,  5.04736153e-02,  9.72188500e-04],\n",
       "       [ 2.14529350e+01,  3.58449808e+00, -1.63261052e+00, ...,\n",
       "         4.65235438e-02, -9.19010628e-03, -1.33753845e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = PreProc(columns, y)\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef61d76-2582-4b9a-8ec1-69cc20233b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(columns, y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b5305a-1e94-4db8-a3cb-91bc36b28379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1705, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train\n",
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "394cf1d6-18c1-47b0-b6c2-a795be989ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1705,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a0c397-d8e6-4fb6-8008-7eec5791a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_Traintemp = pd.DataFrame(X_Train)\n",
    "#Y_Traintemp['Label'] = Y_Train\n",
    "#Y_Traintemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab5d99-5617-43b9-90aa-571ca53627b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2505e7ec-1fa3-4946-86f3-6df24af4a341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1705, 1000)\n",
      "(1705,)\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(X_Train)\n",
    "Y = tf.constant(Y_Train)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d783180f-d64b-49a0-9f40-33795ae3cb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ee8b91-bd9c-4142-a242-6b5b1ca7c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming one timestep per input sample\n",
    "X = tf.reshape(X, (X.shape[0], 1, X.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3538166f-1380-45a7-9283-0cb1849829ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaust\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3698 - loss: 1.1055\n",
      "Epoch 2/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8806 - loss: 0.6958 \n",
      "Epoch 3/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9259 - loss: 0.4896 \n",
      "Epoch 4/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9430 - loss: 0.3687 \n",
      "Epoch 5/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.3215 \n",
      "Epoch 6/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.2683 \n",
      "Epoch 7/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.2137 \n",
      "Epoch 8/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9786 - loss: 0.1788 \n",
      "Epoch 9/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.1457 \n",
      "Epoch 10/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9799 - loss: 0.1301 \n",
      "Epoch 11/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.1063 \n",
      "Epoch 12/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9906 - loss: 0.0914 \n",
      "Epoch 13/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0749 \n",
      "Epoch 14/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0704 \n",
      "Epoch 15/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.0559 \n",
      "Epoch 16/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0509 \n",
      "Epoch 17/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0429 \n",
      "Epoch 18/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9897 - loss: 0.0408 \n",
      "Epoch 19/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9930 - loss: 0.0372 \n",
      "Epoch 20/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9959 - loss: 0.0349 \n",
      "Epoch 21/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9952 - loss: 0.0281 \n",
      "Epoch 22/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9936 - loss: 0.0251 \n",
      "Epoch 23/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9967 - loss: 0.0232 \n",
      "Epoch 24/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.0200 \n",
      "Epoch 25/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9979 - loss: 0.0166 \n",
      "Epoch 26/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9966 - loss: 0.0210 \n",
      "Epoch 27/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0132 \n",
      "Epoch 28/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9994 - loss: 0.0139 \n",
      "Epoch 29/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0118 \n",
      "Epoch 30/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9975 - loss: 0.0110 \n",
      "Epoch 31/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9989 - loss: 0.0122 \n",
      "Epoch 32/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0103 \n",
      "Epoch 33/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0123 \n",
      "Epoch 34/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0078 \n",
      "Epoch 35/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9969 - loss: 0.0098 \n",
      "Epoch 36/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0060 \n",
      "Epoch 37/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0108 \n",
      "Epoch 38/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0073 \n",
      "Epoch 39/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0073 \n",
      "Epoch 40/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0086 \n",
      "Epoch 41/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9962 - loss: 0.0078 \n",
      "Epoch 42/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0074 \n",
      "Epoch 43/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9981 - loss: 0.0054     \n",
      "Epoch 44/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0092 \n",
      "Epoch 45/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0067     \n",
      "Epoch 46/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9972 - loss: 0.0060 \n",
      "Epoch 47/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9948 - loss: 0.0083     \n",
      "Epoch 48/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0046 \n",
      "Epoch 49/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0046 \n",
      "Epoch 50/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0083     \n",
      "Epoch 51/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0046 \n",
      "Epoch 52/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9965 - loss: 0.0057     \n",
      "Epoch 53/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0041     \n",
      "Epoch 54/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9963 - loss: 0.0073 \n",
      "Epoch 55/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0052     \n",
      "Epoch 56/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0075 \n",
      "Epoch 57/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0045     \n",
      "Epoch 58/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9986 - loss: 0.0047     \n",
      "Epoch 59/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9960 - loss: 0.0062     \n",
      "Epoch 60/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9963 - loss: 0.0067     \n",
      "Epoch 61/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9977 - loss: 0.0053 \n",
      "Epoch 62/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0068     \n",
      "Epoch 63/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9981 - loss: 0.0048 \n",
      "Epoch 64/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0060 \n",
      "Epoch 65/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9981 - loss: 0.0045     \n",
      "Epoch 66/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0046     \n",
      "Epoch 67/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9973 - loss: 0.0062     \n",
      "Epoch 68/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0062     \n",
      "Epoch 69/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9988 - loss: 0.0026     \n",
      "Epoch 70/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9930 - loss: 0.0077 \n",
      "Epoch 71/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0041     \n",
      "Epoch 72/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0036     \n",
      "Epoch 73/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0045     \n",
      "Epoch 74/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0064     \n",
      "Epoch 75/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0046     \n",
      "Epoch 76/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0055 \n",
      "Epoch 77/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9980 - loss: 0.0062     \n",
      "Epoch 78/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9950 - loss: 0.0065 \n",
      "Epoch 79/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9990 - loss: 0.0036     \n",
      "Epoch 80/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0034 \n",
      "Epoch 81/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9940 - loss: 0.0073     \n",
      "Epoch 82/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9957 - loss: 0.0050 \n",
      "Epoch 83/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.0042     \n",
      "Epoch 84/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9948 - loss: 0.0063     \n",
      "Epoch 85/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9991 - loss: 0.0045     \n",
      "Epoch 86/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9963 - loss: 0.0063     \n",
      "Epoch 87/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0027 \n",
      "Epoch 88/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0044     \n",
      "Epoch 89/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9975 - loss: 0.0038     \n",
      "Epoch 90/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0031     \n",
      "Epoch 91/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0063     \n",
      "Epoch 92/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0038     \n",
      "Epoch 93/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9974 - loss: 0.0066 \n",
      "Epoch 94/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0037     \n",
      "Epoch 95/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0034     \n",
      "Epoch 96/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0034     \n",
      "Epoch 97/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9998 - loss: 0.0051     \n",
      "Epoch 98/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9998 - loss: 0.0030     \n",
      "Epoch 99/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9992 - loss: 0.0042     \n",
      "Epoch 100/100\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0025     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1bece15b460>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=5, input_shape=(1, 1000)))\n",
    "model.add(tf.keras.layers.Dense(units = 3, activation = 'softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee2c61-9ed1-40f2-a95e-45ba576dd68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26fd3134-383b-419a-a618-2ac304f5261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test = tf.reshape(X_Test, (X_Test.shape[0], 1, X_Test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dccfc687-5991-4b11-9462-e660b350a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f470382-ba1a-4a70-b9d0-577a131d1c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "666     2\n",
       "1869    1\n",
       "1555    1\n",
       "83      2\n",
       "1382    1\n",
       "       ..\n",
       "946     0\n",
       "860     0\n",
       "1979    2\n",
       "1932    2\n",
       "1034    2\n",
       "Name: label, Length: 427, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4744f8ee-5460-4a9d-b96b-f824d9e30ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(427,)\n",
      "(427, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.4409198e-05, 1.2841444e-04, 9.9985719e-01],\n",
       "       [1.4005849e-04, 9.9982256e-01, 3.7396068e-05],\n",
       "       [1.5259956e-04, 9.9980611e-01, 4.1279382e-05],\n",
       "       ...,\n",
       "       [7.2018090e-03, 1.8568981e-03, 9.9094135e-01],\n",
       "       [6.5923462e-07, 3.2826949e-07, 9.9999893e-01],\n",
       "       [2.3535905e-04, 2.5714669e-06, 9.9976200e-01]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y_Test.shape)\n",
    "print(y_pred.shape)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "891c8a81-1036-4a53-a6bd-7195929deaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(1),\n",
       " np.int64(2),\n",
       " np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(2),\n",
       " np.int64(2),\n",
       " np.int64(2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = []\n",
    "for i in y_pred:\n",
    "    check.append(np.where(i==max(i))[0][0])\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a00f41cd-cad8-4638-85a8-0cd31ab665f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95       142\n",
      "           1       0.98      1.00      0.99       128\n",
      "           2       0.98      0.92      0.95       157\n",
      "\n",
      "    accuracy                           0.96       427\n",
      "   macro avg       0.96      0.97      0.96       427\n",
      "weighted avg       0.96      0.96      0.96       427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_Test, check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde7654-6d56-4178-92a6-54fc3398ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60febddf-d524-4eb7-a472-cc3cf872159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "def PreProc(X):\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Columns_Scaled = scaler.transform(X)\n",
    "    pca = PCA(n_components=1000).fit(Columns_Scaled).transform(Columns_Scaled)\n",
    "    return pca\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"emotions.csv\")\n",
    "columns = df.drop(\"label\", axis=1)\n",
    "y = df[\"label\"].replace([\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"], [0, 1,2])\n",
    "\n",
    "columns = PreProc(columns)\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(columns, y, test_size=0.2)\n",
    "\n",
    "# Convert to tensors\n",
    "X = tf.constant(X_Train, dtype=tf.float32)\n",
    "Y = tf.constant(Y_Train, dtype=tf.int32)\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X = tf.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=5, input_shape=(1, 1000)))\n",
    "model.add(tf.keras.layers.Dense(units=5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=50)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52946e-eb24-4179-b31e-7a18348f550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ba33c-fbec-4a61-9fe7-9a6eb9351ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26abf5-2d2e-46ce-ae73-ed86c24310d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(units=5, input_shape=(1, 1000)))\n",
    "#model.add(tf.keras.layers.Dense(units = 5, activation = 'softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, Y, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
